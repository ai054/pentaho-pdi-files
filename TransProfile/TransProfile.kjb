<?xml version="1.0" encoding="UTF-8"?>
<job>
  <name>TransProfile</name>
  <description/>
  <extended_description/>
  <job_version/>
  <job_status>0</job_status>
  <directory>/</directory>
  <created_user>-</created_user>
  <created_date>2017/04/14 09:39:12.426</created_date>
  <modified_user>-</modified_user>
  <modified_date>2017/04/14 09:39:12.426</modified_date>
  <parameters>
    <parameter>
      <name>HOST</name>
      <default_value/>
      <description/>
    </parameter>
    <parameter>
      <name>account</name>
      <default_value/>
      <description/>
    </parameter>
  </parameters>
  <connection>
    <name>hadoop</name>
    <server>${HOST}</server>
    <type>HIVE2</type>
    <access>Native</access>
    <database>feeds</database>
    <port>10000</port>
    <username>hive</username>
    <password>Encrypted 2be98afc86aa7f2e4cb79ce10d69bb9df</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>10000</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SQL_CONNECT</code>
        <attribute>set hive.execution.engine=mr;</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>N</attribute>
      </attribute>
    </attributes>
  </connection>
  <slaveservers>
    </slaveservers>
  <job-log-table>
    <connection/>
    <schema/>
    <table/>
    <size_limit_lines/>
    <interval/>
    <timeout_days/>
    <field>
      <id>ID_JOB</id>
      <enabled>Y</enabled>
      <name>ID_JOB</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>JOBNAME</name>
    </field>
    <field>
      <id>STATUS</id>
      <enabled>Y</enabled>
      <name>STATUS</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>STARTDATE</id>
      <enabled>Y</enabled>
      <name>STARTDATE</name>
    </field>
    <field>
      <id>ENDDATE</id>
      <enabled>Y</enabled>
      <name>ENDDATE</name>
    </field>
    <field>
      <id>LOGDATE</id>
      <enabled>Y</enabled>
      <name>LOGDATE</name>
    </field>
    <field>
      <id>DEPDATE</id>
      <enabled>Y</enabled>
      <name>DEPDATE</name>
    </field>
    <field>
      <id>REPLAYDATE</id>
      <enabled>Y</enabled>
      <name>REPLAYDATE</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>Y</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>EXECUTING_SERVER</id>
      <enabled>N</enabled>
      <name>EXECUTING_SERVER</name>
    </field>
    <field>
      <id>EXECUTING_USER</id>
      <enabled>N</enabled>
      <name>EXECUTING_USER</name>
    </field>
    <field>
      <id>START_JOB_ENTRY</id>
      <enabled>N</enabled>
      <name>START_JOB_ENTRY</name>
    </field>
    <field>
      <id>CLIENT</id>
      <enabled>N</enabled>
      <name>CLIENT</name>
    </field>
  </job-log-table>
  <jobentry-log-table>
    <connection/>
    <schema/>
    <table/>
    <timeout_days/>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>JOBNAME</id>
      <enabled>Y</enabled>
      <name>TRANSNAME</name>
    </field>
    <field>
      <id>JOBENTRYNAME</id>
      <enabled>Y</enabled>
      <name>STEPNAME</name>
    </field>
    <field>
      <id>LINES_READ</id>
      <enabled>Y</enabled>
      <name>LINES_READ</name>
    </field>
    <field>
      <id>LINES_WRITTEN</id>
      <enabled>Y</enabled>
      <name>LINES_WRITTEN</name>
    </field>
    <field>
      <id>LINES_UPDATED</id>
      <enabled>Y</enabled>
      <name>LINES_UPDATED</name>
    </field>
    <field>
      <id>LINES_INPUT</id>
      <enabled>Y</enabled>
      <name>LINES_INPUT</name>
    </field>
    <field>
      <id>LINES_OUTPUT</id>
      <enabled>Y</enabled>
      <name>LINES_OUTPUT</name>
    </field>
    <field>
      <id>LINES_REJECTED</id>
      <enabled>Y</enabled>
      <name>LINES_REJECTED</name>
    </field>
    <field>
      <id>ERRORS</id>
      <enabled>Y</enabled>
      <name>ERRORS</name>
    </field>
    <field>
      <id>RESULT</id>
      <enabled>Y</enabled>
      <name>RESULT</name>
    </field>
    <field>
      <id>NR_RESULT_ROWS</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_ROWS</name>
    </field>
    <field>
      <id>NR_RESULT_FILES</id>
      <enabled>Y</enabled>
      <name>NR_RESULT_FILES</name>
    </field>
    <field>
      <id>LOG_FIELD</id>
      <enabled>N</enabled>
      <name>LOG_FIELD</name>
    </field>
    <field>
      <id>COPY_NR</id>
      <enabled>N</enabled>
      <name>COPY_NR</name>
    </field>
  </jobentry-log-table>
  <channel-log-table>
    <connection/>
    <schema/>
    <table/>
    <timeout_days/>
    <field>
      <id>ID_BATCH</id>
      <enabled>Y</enabled>
      <name>ID_BATCH</name>
    </field>
    <field>
      <id>CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>CHANNEL_ID</name>
    </field>
    <field>
      <id>LOG_DATE</id>
      <enabled>Y</enabled>
      <name>LOG_DATE</name>
    </field>
    <field>
      <id>LOGGING_OBJECT_TYPE</id>
      <enabled>Y</enabled>
      <name>LOGGING_OBJECT_TYPE</name>
    </field>
    <field>
      <id>OBJECT_NAME</id>
      <enabled>Y</enabled>
      <name>OBJECT_NAME</name>
    </field>
    <field>
      <id>OBJECT_COPY</id>
      <enabled>Y</enabled>
      <name>OBJECT_COPY</name>
    </field>
    <field>
      <id>REPOSITORY_DIRECTORY</id>
      <enabled>Y</enabled>
      <name>REPOSITORY_DIRECTORY</name>
    </field>
    <field>
      <id>FILENAME</id>
      <enabled>Y</enabled>
      <name>FILENAME</name>
    </field>
    <field>
      <id>OBJECT_ID</id>
      <enabled>Y</enabled>
      <name>OBJECT_ID</name>
    </field>
    <field>
      <id>OBJECT_REVISION</id>
      <enabled>Y</enabled>
      <name>OBJECT_REVISION</name>
    </field>
    <field>
      <id>PARENT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>PARENT_CHANNEL_ID</name>
    </field>
    <field>
      <id>ROOT_CHANNEL_ID</id>
      <enabled>Y</enabled>
      <name>ROOT_CHANNEL_ID</name>
    </field>
  </channel-log-table>
  <pass_batchid>N</pass_batchid>
  <shared_objects_file/>
  <entries>
    <entry>
      <name>START</name>
      <description/>
      <type>SPECIAL</type>
      <start>Y</start>
      <dummy>N</dummy>
      <repeat>N</repeat>
      <schedulerType>0</schedulerType>
      <intervalSeconds>0</intervalSeconds>
      <intervalMinutes>60</intervalMinutes>
      <hour>12</hour>
      <minutes>0</minutes>
      <weekDay>1</weekDay>
      <DayOfMonth>1</DayOfMonth>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>32</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>GetAttributeList</name>
      <description/>
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id/>
      <filename>${Internal.Job.Filename.Directory}/read_att.ktr</filename>
      <transname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile/>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name/>
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
        <parameter>
          <name>account</name>
          <stream_name/>
          <value>${account}</value>
        </parameter>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>720</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>Success</name>
      <description/>
      <type>SUCCESS</type>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>624</xloc>
      <yloc>192</yloc>
    </entry>
    <entry>
      <name>trans_profile</name>
      <description/>
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id/>
      <filename>${Internal.Job.Filename.Directory}/trans_profile_attribute_level.ktr</filename>
      <transname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile/>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name/>
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>816</xloc>
      <yloc>16</yloc>
    </entry>
    <entry>
      <name>SQL</name>
      <description/>
      <type>SQL</type>
      <sql>
create table transaction_profile${account} as
select a.customer_id, a.transdate,tm_btw_tx as date_diff,cum_custage, order_rank,prodage,round(profit,2) as profit,round(cum_profit,2) as cum_profit,att_name,att_value,cal_value from trans_base a
join time_bw_txn b on a.customer_id=b.customer_id and a.transdate=b.transdate
join trans_order_profit c on a.customer_id=c.customer_id and a.transdate=c.transdate
join trans_prod_age d on a.customer_id=d.customer_id and a.transdate=d.transdate
left join trans_profile_cal e on a.customer_id=e.customer_id and a.transdate=e.trans_date
group by a.customer_id, a.transdate,tm_btw_tx,cum_custage, order_rank,prodage,profit,cum_profit,att_name,att_value,cal_value
order by a.customer_id, a.transdate</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>hadoop</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>992</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>SQL 2</name>
      <description/>
      <type>SQL</type>
      <sql>drop table if exists trans_base;
create table trans_base as select order_id,customer_id,transdate,cost,unitprice,quantity,channel,b.* from orderinfo a, productinfo b  where a.product_id=b.product_id ;


drop table if exists trans_date;
create  table trans_date as
select distinct customer_id, transdate from trans_base;

drop table if exists trans_date_lead;
create table trans_date_lead as
select customer_id, transdate, lead(transdate,1) over (PARTITION BY customer_id order by transdate) as lead_date ,lag(transdate,1) over (PARTITION BY customer_id order by transdate) as lag_date,row_number() over (partition by  customer_id order by transdate ) as order_rank
from trans_date where customer_id is not null;
drop table if exists time_bw_txn;
create  table time_bw_txn as
select customer_id, transdate,datediff(lead_date,transdate) as tm_btw_tx, if(order_rank=1,0,sum(datediff(transdate,lag_date)) over(partition by customer_id order by transdate)) as cum_custAge,order_rank from trans_date_lead;
drop table if exists bucket;
create  table bucket AS
select if((q1-1.5*(q3-q1)) &lt; 0,0,(q1-1.5*(q3-q1))) as min,(q3+1.5*(q3-q1)) as max,((q3+1.5*(q3-q1)) -if((q1-1.5*(q3-q1)) &lt; 0,0,(q1-1.5*(q3-q1))))/9 as b_size
from 
(select  percentile(cast(tm_btw_tx as BIGINT),0.25) as q1,percentile(cast(tm_btw_tx as BIGINT),0.75) as q3 from time_bw_txn where tm_btw_tx is  not null) a;

drop table if exists trans_order_profit;
create table trans_order_profit as 
select customer_id,transdate,profit,sum(profit) over(partition by customer_id order by transdate) as cum_profit from 
(select customer_id, transdate, (sum(price)-sum(cost) ) as profit from
(select customer_id, transdate,product_id, unitprice*quantity as price, cost*quantity as cost from trans_base)a 
where customer_id is not null group by customer_id, transdate) b;

drop table if exists trans_prod_age;
create table trans_prod_age as 
select customer_id, transdate, avg(quantity*prod_age) as ProdAge from 
(select customer_id, transdate, product_id,quantity, datediff(current_date, first_transdate) as prod_age from 
(select customer_id, transdate, product_id,quantity,current_date ,min(transdate) over (partition by product_id ) as first_transdate from trans_base 
where customer_id is not null) a) b
group by customer_id, transdate;


drop table if exists bucket_no;
create table  bucket_no as

select min, 1*b_size as max, 'B1' as bucket_no from bucket
union
select 1*b_size as min,2*b_size as max,'B2' as bucket_no from bucket
union
select 2*b_size as min,3*b_size as max,'B3' as bucket_no from bucket
union
select 3*b_size as min,4*b_size as max,'B4' as bucket_no from bucket
union
select 4*b_size as min,5*b_size as max,'B5' as bucket_no from bucket
union
select 5*b_size as min,6*b_size as max,'B6' as bucket_no from bucket
union
select 6*b_size as min,7*b_size as max,'B7' as bucket_no from bucket
union
select 7*b_size as min,8*b_size as max,'B8' as bucket_no from bucket
union
select 8*b_size as min,9*b_size as max,'B9' as bucket_no from bucket
union
select 9*b_size as min,max,'B10' as bucket_no from bucket;

</sql>
      <useVariableSubstitution>F</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>hadoop</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>624</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>orderinfo_step1</name>
      <description/>
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id/>
      <filename>${Internal.Job.Filename.Directory}/CreateBaseTable.ktr</filename>
      <transname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile/>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name/>
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
        <parameter>
          <name>account</name>
          <stream_name/>
          <value>${account}</value>
        </parameter>
        <parameter>
          <name>HOST</name>
          <stream_name/>
          <value>${HOST}</value>
        </parameter>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>192</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>orderinfo</name>
      <description/>
      <type>SQL</type>
      <sql>create table orderinfo as select order_id,a.customer_id, a.trans_date as transdate, product_id,quantity,unitprice,unitcost as cost,channel from orderinfo_step0 a left join total_order_channel b on a.order_id=b.order_id where quantity &lt;&gt; 0 group by a.order_id,a.customer_id, a.trans_date, product_id,quantity,unitprice,unitcost,channel</sql>
      <useVariableSubstitution>F</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>T</sendOneStatement>
      <connection>hadoop</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>544</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>SQL 3</name>
      <description/>
      <type>SQL</type>
      <sql>drop table if exists orderinfo;
create table order_channel as
select order_id, channel from order_analytics_idx a  
join campaigntype${account} b on a.campaign_id=b.campaign_id
union
select a.order_id, b.channel from 
(select order_id ,campaign_id,campaign_name from order_analytics_idx where campaign_id='(not set)' and campaign_name &lt;&gt; '(not set)') a
join campaigntype${account} b on lower(a.campaign_name)=lower(b.campaign_name);

create table total_order_channel as 
select  a.order_id,coalesce(coalesce(channel,case when medium='cpc' then concat(source,'-','medium') else medium end),'offline') as channel from order_analytics_idx a  
left join order_channel b on a.order_id=b.order_id;</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>hadoop</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>480</xloc>
      <yloc>128</yloc>
    </entry>
    <entry>
      <name>SQL 4</name>
      <description/>
      <type>SQL</type>
      <sql>drop table if exists orderinfo_step0;
drop table if exists customerinfo;
drop table if exists productinfo;
drop table if exists order_analytics_idx;
drop table if exists order_channel;
drop table if exists total_order_channel;
drop table if exists analytics;
drop table if exists order_profit;
drop table if exists trans_profile_cal;
drop table if exists trans_profile_pivot${account};
drop table if exists transaction_profile${account};</sql>
      <useVariableSubstitution>T</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>hadoop</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>112</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>dropIntermediateTable</name>
      <description/>
      <type>SQL</type>
      <sql>drop table if exists trans_base;
drop table if exists trans_date;
drop table if exists trans_date_lead;
drop table if exists time_bw_txn;
drop table if exists trans_order_profit;
drop table if exists trans_prod_age;
drop table if exists bucket_no;
drop table if exists orderinfo;
drop table if exists orderinfo_step0;
drop table if exists customerinfo;
drop table if exists productinfo;
</sql>
      <useVariableSubstitution>F</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>hadoop</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>992</xloc>
      <yloc>176</yloc>
    </entry>
    <entry>
      <name>OrderAnalytics</name>
      <description/>
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id/>
      <filename>${Internal.Job.Filename.Directory}/CreateOrderAnalytics.ktr</filename>
      <transname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile/>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name/>
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
        <parameter>
          <name>HOST</name>
          <stream_name/>
          <value>${HOST}</value>
        </parameter>
        <parameter>
          <name>account</name>
          <stream_name/>
          <value>${account}</value>
        </parameter>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>288</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>OrderAnalyticsCreate</name>
      <description/>
      <type>SQL</type>
      <sql>create table order_analytics_idx as select a.* ,b.campaign_id,b.adgroup_id,b.campaign_name,b.adgroup_name,b.keyword,b.source,b.medium from order_profit a left join analytics b on a.order_id=b.transaction_id;</sql>
      <useVariableSubstitution>F</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>hadoop</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>384</xloc>
      <yloc>32</yloc>
    </entry>
    <entry>
      <name>trans_profile_cal</name>
      <description/>
      <type>SQL</type>
      <sql>create external table trans_profile_cal(customer_id string,trans_date string, att_name string,att_value string ,cal_value double) row format delimited fields  terminated by ',' stored as textfile location 's3://tatras-octopi/trans_profile_cal';

</sql>
      <useVariableSubstitution>F</useVariableSubstitution>
      <sqlfromfile>F</sqlfromfile>
      <sqlfilename/>
      <sendOneStatement>F</sendOneStatement>
      <connection>hadoop</connection>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>912</xloc>
      <yloc>16</yloc>
    </entry>
    <entry>
      <name>Pivoting</name>
      <description/>
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id/>
      <filename>${Internal.Job.Filename.Directory}/pivot_inject.ktr</filename>
      <transname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile/>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name/>
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
        <parameter>
          <name>HOST</name>
          <stream_name/>
          <value>${HOST}</value>
        </parameter>
        <parameter>
          <name>account</name>
          <stream_name/>
          <value>${account}</value>
        </parameter>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>880</xloc>
      <yloc>160</yloc>
    </entry>
    <entry>
      <name>FinalTable</name>
      <description/>
      <type>TRANS</type>
      <specification_method>filename</specification_method>
      <trans_object_id/>
      <filename>${Internal.Job.Filename.Directory}/CreateFinalTable.ktr</filename>
      <transname/>
      <arg_from_previous>N</arg_from_previous>
      <params_from_previous>N</params_from_previous>
      <exec_per_row>N</exec_per_row>
      <clear_rows>N</clear_rows>
      <clear_files>N</clear_files>
      <set_logfile>N</set_logfile>
      <logfile/>
      <logext/>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <loglevel>Basic</loglevel>
      <cluster>N</cluster>
      <slave_server_name/>
      <set_append_logfile>N</set_append_logfile>
      <wait_until_finished>Y</wait_until_finished>
      <follow_abort_remote>N</follow_abort_remote>
      <create_parent_folder>N</create_parent_folder>
      <logging_remote_work>N</logging_remote_work>
      <run_configuration>Pentaho local</run_configuration>
      <parameters>
        <pass_all_parameters>Y</pass_all_parameters>
        <parameter>
          <name>HOST</name>
          <stream_name/>
          <value>${HOST}</value>
        </parameter>
        <parameter>
          <name>account</name>
          <stream_name/>
          <value>${account}</value>
        </parameter>
      </parameters>
      <parallel>N</parallel>
      <draw>Y</draw>
      <nr>0</nr>
      <xloc>752</xloc>
      <yloc>176</yloc>
    </entry>
  </entries>
  <hops>
    <hop>
      <from>orderinfo</from>
      <to>SQL 2</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>SQL 2</from>
      <to>GetAttributeList</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>GetAttributeList</from>
      <to>trans_profile</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>START</from>
      <to>SQL 4</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>Y</unconditional>
    </hop>
    <hop>
      <from>SQL 4</from>
      <to>orderinfo_step1</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>SQL</from>
      <to>dropIntermediateTable</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>OrderAnalytics</from>
      <to>OrderAnalyticsCreate</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>orderinfo_step1</from>
      <to>OrderAnalytics</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>OrderAnalyticsCreate</from>
      <to>SQL 3</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>SQL 3</from>
      <to>orderinfo</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>trans_profile</from>
      <to>trans_profile_cal</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>trans_profile_cal</from>
      <to>SQL</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>dropIntermediateTable</from>
      <to>Pivoting</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>Pivoting</from>
      <to>FinalTable</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
    <hop>
      <from>FinalTable</from>
      <to>Success</to>
      <from_nr>0</from_nr>
      <to_nr>0</to_nr>
      <enabled>Y</enabled>
      <evaluation>Y</evaluation>
      <unconditional>N</unconditional>
    </hop>
  </hops>
  <notepads>
  </notepads>
  <attributes>
    <group>
      <name>METASTORE.NamedClusters</name>
      <attribute>
        <key>NamedCluster</key>
        <value>{"namespace":"NamedClusters","id":"NamedCluster","name":"NamedCluster","description":"A NamedCluster","metaStoreName":null}</value>
      </attribute>
    </group>
    <group>
      <name>METASTORE.pentaho</name>
      <attribute>
        <key>Default Run Configuration</key>
        <value>{"namespace":"pentaho","id":"Default Run Configuration","name":"Default Run Configuration","description":"Defines a default run configuration","metaStoreName":null}</value>
      </attribute>
    </group>
    <group>
      <name>{"_":"Embedded MetaStore Elements","namespace":"NamedClusters","type":"NamedCluster"}</name>
      <attribute>
        <key>hadoop</key>
        <value>{"children":[{"children":[],"id":"hdfsPassword","value":null},{"children":[],"id":"oozieUrl","value":"http://node-01.algo:8080/oozie"},{"children":[],"id":"mapr","value":"N"},{"children":[],"id":"useGateway","value":"N"},{"children":[],"id":"lastModifiedDate","value":"1500013133689"},{"children":[],"id":"jobTrackerHost","value":"node-01.algo"},{"children":[],"id":"zooKeeperHost","value":"node-01.algo"},{"children":[],"id":"shimIdentifier","value":null},{"children":[],"id":"gatewayUrl","value":null},{"children":[],"id":"jobTrackerPort","value":"8050"},{"children":[],"id":"zooKeeperPort","value":"2181"},{"children":[],"id":"name","value":"hadoop"},{"children":[],"id":"hdfsPort","value":"8020"},{"children":[],"id":"hdfsUsername","value":"TD054"},{"children":[],"id":"gatewayPassword","value":null},{"children":[],"id":"kafkaBootstrapServers","value":null},{"children":[],"id":"storageScheme","value":"hdfs"},{"children":[],"id":"hdfsHost","value":"node-01.algo"},{"children":[],"id":"gatewayUsername","value":null}],"id":"hadoop","value":null,"name":"hadoop","owner":null,"ownerPermissionsList":[]}</value>
      </attribute>
    </group>
    <group>
      <name>{"_":"Embedded MetaStore Elements","namespace":"pentaho","type":"Default Run Configuration"}</name>
      <attribute>
        <key>Pentaho local</key>
        <value>{"children":[{"children":[],"id":"server","value":null},{"children":[],"id":"clustered","value":"N"},{"children":[],"id":"name","value":"Pentaho local"},{"children":[],"id":"description","value":null},{"children":[],"id":"pentaho","value":"N"},{"children":[],"id":"readOnly","value":"Y"},{"children":[],"id":"sendResources","value":"N"},{"children":[],"id":"logRemoteExecutionLocally","value":"N"},{"children":[],"id":"remote","value":"N"},{"children":[],"id":"local","value":"Y"},{"children":[],"id":"showTransformations","value":"N"}],"id":"Pentaho local","value":null,"name":"Pentaho local","owner":null,"ownerPermissionsList":[]}</value>
      </attribute>
    </group>
  </attributes>
</job>
